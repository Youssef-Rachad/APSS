# -*- coding: utf-8 -*-
"""batchingV7_functionsonly.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tE9O_1O6A_ShV3YCSB7ktg_Q2jfsFIqO

### Batching Protocols from Data Files
**USING 4bp DATA**

- here were are trying to develop a way to balance the data such that orders of magnitude are roughly in line with one another.
- We are finding in training that the overwhelming amounts of zeros in our dataset are causing issues, so balancing remains an important step
- Here we want to scale up amounts, but also keeping in mind that dividing our data into testing and validation means we can not have overlap between these two steps.
- this is accomplished by scaling the testing dataset only, before batching is complete

AUTHOR(S): Youssef, John Wolf, Zoe Kutulakos DATE: 03/29/2023
    
    - Zoe added shuffling of balanced train indices in balance_train_idxs() function before the return statement
    - Zoe used 4bp data
    - Organized functions vs print-outs
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
import os
from copy import deepcopy

from torch.utils.data.sampler import SubsetRandomSampler

data_file_name = 'FileP04_T4_18h_37C.xlsx'


################## RETRIEVE DATA FROM EXCEL FILE ##################
def get_data_matrix(data_file_name): 
  '''INPUT: File name (should be in the same directory as this file)
     OUTPUT: a pandas df file that can access intensities via df["AAA"]["TTT"]'''


  # pandas dataframe [this is the matrix right from xlsx file]
  df = pd.read_excel(data_file_name)
  # this is changing the index to TTT or AAA as we would like
  df.set_index(df.columns[0], inplace = True)
  df_num_aff = df.astype(int, copy=True, errors='raise')
  total_num_bind_events = df_num_aff.sum().sum()
  return df_num_aff


################## GET DATA PAIRINGS FROM PANDAS DATAFRAME ##################

def get_data_pairings(data_matrix):
  '''INPUT:  A pandas dataframe containing integer values representing
             number of binding events between two DNA sequences
     OUTPUT: A list of lists of the form [["AAA", "TTT", num_binding_events0],
                                                          ...
                                          ["AAC", "TTG", num_binding_events1]]
             where num_binding_events is of type np.int64'''
  output = []
  for pair_1 in data_matrix.columns: 
    for pair_2 in data_matrix.index:
      bind_events = data_matrix[pair_1][pair_2]
      #assert type(bind_events) == np.int64
      pair = [pair_1, pair_2, bind_events]
      output.append(pair)
  return output # this is a list


################## NORMALIZE NUM. BINDING EVENTS TO AFFINITY IN DATA PAIRINGS ##################

def normalized_data_pairings(list_pairs):
    '''INPUT: A list of the data in full dataset, of the form:
            [["AAA", "TTT", num_binding_events0],
                            ...
             ["AAC", "TTG", num_binding_events1]]
     OUTPUT: a normalized tuple array of the data (helps with comparision)'''
    norm_list_pairs = []
    total_binding_events = np.array(list_pairs)[:,2].astype(int).sum()
    for pair in list_pairs:
      affinity = float(pair[2]/total_binding_events) 
      #assert type(affinity) == float
      norm_list_pairs.append([pair[0], pair[1], affinity])
    return norm_list_pairs

def normalized_data_pairings_v2(list_pairs):
    total = sum(item[2] for item in list_pairs)
    return [[item[0], item[1], (item[2])/total] for item in list_pairs]

################## REPRESENT DATA PAIRINGS WITH EMBEDDED TOKENS ##################

def convert_alpha_sequence(data_pairings): #Takes data_pairings, normalised or not
  vocab = {'A': np.array([1, 0]), 'T': np.array([0, 1]), 
             'C': np.array([0, 0]), 'G': np.array([1, 1])}
  embedded = []
  for data_pairing in range(len(data_pairings)):
    sequence_1 = np.array(list(map(lambda x: vocab[x], data_pairings[data_pairing][0])))
    sequence_2 = np.array(list(map(lambda x: vocab[x], data_pairings[data_pairing][1])))
    affinity   = data_pairings[data_pairing][2]
    embedded.append((sequence_1, sequence_2, affinity))

  return embedded

################## GET INDICES FOR TRAIN, VALIDATION, AND TEST SETS (SPLITTING FULL LIST OF DATA PAIRINGS INTO THREE SUBSETS) ##################

def train_val_test_split(dataset, split_proportions, split_seed=1000):
    '''
    INPUTS:
      dataset: This is the full dataset in the format output by 
               convert_alpha_sequence(), i.e., ########## WHAT FORMAT IS THIS?
      split_proportions: Proportions with which we want to split our full 
                         dataset into training, validation, and test sets.
                         Format is: [train_proportion, val_proportion, test_proportion],
                         where train_proportion, val_proportion, test_proportion
                         are all floats between 0 and 1. 
                         The three proportions must add up to 1.
      split_seed: seed that will be set for reproducible splitting.
    '''
    np.random.seed(split_seed)

    # Get random indices for train/val/test set samples:
    total_len = len(dataset) # total number of indices
    indices = np.arange(total_len)
    np.random.shuffle(indices)
    train_val_split = int((split_proportions[0]) * total_len)
    val_test_split = train_val_split + int((split_proportions[1]) * total_len)
    train_idxs = indices[:train_val_split] 
    # indices for samples in training set
    val_idxs = indices[train_val_split:val_test_split]
    # indices for samples in validation set
    test_idxs = indices[val_test_split:]

    #return the indices in the dataset array which will compose of the train, validation and test
    return train_idxs, val_idxs, test_idxs

################## HELPER FUNCTIONS FOR DATA BALANCING ##################

def get_magnitude_dict(train_idxs, data_pairings):
    '''INPUT:  A list of indices in the training set, and the full dataset of pairings
       OUTPUT: A dictionary that contains a list of train indices keyed by their order
       of magnitude ex: "10e-4" or "Zero Value"
    '''
    magnitude_dict = {}

    for train_idx in train_idxs:
        pairing = data_pairings[train_idx]
        binding_affinity = pairing[2]
        affinity_magnitude = "Zero Value" if binding_affinity == 0 else "10e" +  str(int(np.floor(np.log10(binding_affinity))))
        if affinity_magnitude in magnitude_dict:
            magnitude_dict[affinity_magnitude].append(train_idx)
        else:
            magnitude_dict[affinity_magnitude] = [train_idx]    
    return magnitude_dict

def plot_magnitude_dict(magnitude_dict):
    '''INPUT: A dictionary that contains a list of train indices keyed by their order
       of magnitude ex: "10e-4" or "Zero Value"
       OUTPUT: None, creates a histogram of all the indices based on their order of magnitude
    '''
    values = sorted(list(magnitude_dict.keys()))
    zero_value = values.pop()
    values.insert(0, zero_value)
    frequencies = [len(magnitude_dict[k]) for k in values]

    values, bins, bars = plt.hist(values, bins=len(values), weights=frequencies)
    # Add frequency labels to the bars
    plt.bar_label(bars)

    plt.xlabel('Order of Magnitude')
    plt.ylabel('Frequency')
    plt.title('Histogram of Binding Affinity Order of Magnitudes')
    plt.show()

def balance_train_idxs(train_idxs, data_pairings, shuffle_seed=56, plot=False):
    '''INPUT:  A list of indices in the training set
       OUTPUT: A list of indices in the training set with duplicates to balance
               the number of occurrences of each order magnitude
    '''
    
    magnitude_dict = get_magnitude_dict(train_idxs, data_pairings)
    # print([(k, len(magnitude_dict[k])) for k in magnitude_dict])
    if plot:
        plot_magnitude_dict(magnitude_dict)
       
    # Assume that zero value is always the maximum for now
    balanced_train_idxs = list(deepcopy(train_idxs))
    max_freq = len(magnitude_dict["Zero Value"]) # max_freq = *number* of samples of magnitude 0?
    for magnitude in magnitude_dict:
        if magnitude != "Zero Value":
            cur_freq = len(magnitude_dict[magnitude]) # cur_freq = *number* of samples of current magnitude?
            multiple = round(max_freq / cur_freq)
            for i in range(multiple - 1):
                balanced_train_idxs.extend(magnitude_dict[magnitude])
    
    # ZOE ADDED THIS
    np.random.seed(shuffle_seed)
    np.random.shuffle(balanced_train_idxs)

    if plot:
        new_magnitude_dict = get_magnitude_dict(balanced_train_idxs, data_pairings)
        # print([(k, len(new_magnitude_dict[k])) for k in new_magnitude_dict])
        plot_magnitude_dict(new_magnitude_dict)
    
    return np.array(balanced_train_idxs)

def batching_fcn(dataset, split_proportion, batch_size, split_seed=1000, shuffle_seed=56, batching_seed=42, balance = True):
    # Get dataset split
    unb_train_ind, val_ind, test_ind = train_val_test_split(dataset, split_proportion, split_seed)
    # balancing the train indices ONLY, val and testing are left alone
    if balance == True: 
      train_ind = balance_train_idxs(unb_train_ind, dataset, shuffle_seed, plot=False) # balanced train indices
    else:
      train_ind = unb_train_ind
    # Define a sampler and a dataloader to batch data
    np.random.seed(batching_seed)
    train_sampler = SubsetRandomSampler(train_ind)
    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                               num_workers=1, sampler=train_sampler)
    val_sampler = SubsetRandomSampler(val_ind)
    val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                              num_workers=1, sampler=val_sampler)
    test_sampler = SubsetRandomSampler(test_ind)
    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                             num_workers=1, sampler=test_sampler)
    return train_loader, val_loader, test_loader
