{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpnzRIAeFiUo"
   },
   "source": [
    "### GRU and ANN Training\n",
    "\n",
    "- Here I am going to experiment on the 3bp sequences\n",
    "- again, I want to see if overfitting can occur\n",
    "- I am using balancing, and am taking only positive non zero parts of the dataset for now\n",
    "\n",
    "AUTHORS: Youssef Rachad\n",
    "DATE: 03/31/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0vsOsRQg6OOH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dv-1yANG8kOp"
   },
   "outputs": [],
   "source": [
    "use_cuda = True \n",
    "device_id = 0  # choose the GPU device ID you want to use\n",
    "device = torch.device(f'cuda:{device_id}' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMupHVe4gYJD"
   },
   "source": [
    "#### Importing Batching Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dtTzWXBwgXme"
   },
   "outputs": [],
   "source": [
    "# custome made batching files to feed into our network (in file called batching.py)\n",
    "from batchingv7 import get_data_matrix, get_data_pairings,convert_alpha_sequence, batching_fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hckCFVtU1nCl",
    "outputId": "ada1f4fb-efcd-46ca-c6ba-cdcd9db79461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "# THIS IS WHERE THE DATA FILE IS IN THE FOLDER DOCUMENT\n",
    "data_file_name = 'FileP04_T4_18h_37C.xlsx'\n",
    "\n",
    "#importing the file and getting pairing\n",
    "data_pairings = get_data_pairings(get_data_matrix(data_file_name)) \n",
    "\n",
    "import random\n",
    "\n",
    "norm_pairings = data_pairings\n",
    "\n",
    "# create imbeddings \n",
    "embedded = convert_alpha_sequence(norm_pairings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "I4klF1M66fAI"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def get_abs_percent_error(y_pred, y_actual):\n",
    "  result = torch.mean(torch.abs((y_actual - y_pred) / y_actual))\n",
    "  #ipdb.set_trace()\n",
    "  return result\n",
    "\n",
    "def get_smape(y_pred, y_actual):\n",
    "  batch_smape = torch.mean(torch.abs(y_actual - y_pred) / ((torch.abs(y_actual) + torch.abs(y_pred))/2.0))\n",
    "  # STILL POSSIBILITY THAT BOTH y_pred AND y_actual are 0 (e.g., if our model \n",
    "  # makes very good predictions), and therefore get inf% error\n",
    "  # ==> THINK ABOUT HOW TO FIX THIS MORE PERMANENTLY\n",
    "  return batch_smape\n",
    "\n",
    "def plot_losses(iterations, training_loss, validation_loss, path):\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iterations, training_loss, label=\"Training\")\n",
    "    plt.plot(iterations, validation_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(path)\n",
    "    plt.clf()\n",
    "\n",
    "# Note: MAPE is Mean Absolute Percentage Error\n",
    "def plot_SMAPE(iterations, training_SMAPE, validation_SMAPE, path):\n",
    "    plt.title(\"Training Curve - SMAPE\")\n",
    "    plt.plot(iterations, training_SMAPE, label=\"Training\")\n",
    "    plt.plot(iterations, validation_SMAPE, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Symmetric Mean Average Percent Error\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(path)\n",
    "    plt.clf()\n",
    "\n",
    "def train(modelGru1, modelGru2, modelAnn, training_set, validation_set, batch_size, num_epochs=5, learning_rate=1e-4, plot=True, path=\".\"):\n",
    "    '''\n",
    "    modelGru1 : GRU for analysis of the top sequence\n",
    "    modelGru2 : GRU for analysis of the bottom sequence\n",
    "    modelAnn: For classiciation of the hidden state of the GRU\n",
    "    training_set: Batched [formatted properly] training set\n",
    "    validation_set: Batched [formatted properly] validation data\n",
    "    '''\n",
    "\n",
    "    # using mean squared error loss as this is not a classification problem\n",
    "    criterion = nn.MSELoss()\n",
    "    # NOTE: Might need to change depending on how the second model plays a role here\n",
    "    optimizer = optim.Adam(list(modelGru2.parameters()) +list(modelGru1.parameters()) + list(modelAnn.parameters()), lr= learning_rate) ## insert reference? \n",
    "\n",
    "    # recording the data\n",
    "    training_losses, validation_losses = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    training_SMAPE, validation_SMAPE = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    iterations = range(num_epochs)\n",
    "    \n",
    "    date_time = datetime.now().strftime(\"%m_%d_%H_%M\")\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    file_data = open(f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{num_epochs}_dropout{modelAnn.p}.txt\", \"a\")\n",
    "    file_data.write(f\"Chose learning rate {learning_rate}, number of epochs {num_epoch}, hidden_size {hidden_size}, batch size {batch_size}, dropout {dropout_p}\\n\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      # first sequence, second sequence, and then the affinity (ie the label)\n",
    "\n",
    "      train_total_loss = 0\n",
    "      val_total_loss = 0\n",
    "      train_total_error = 0\n",
    "      val_total_error = 0\n",
    "\n",
    "      num_train_batches = 0\n",
    "      num_val_batches = 0\n",
    "\n",
    "      # TRAINING LOOP\n",
    "      for seq1, seq2, label in training_set:\n",
    "        num_train_batches += 1\n",
    "      \n",
    "        # Forward pass through the two GRU (applied to the first and second sequency)\n",
    "        out, h1 = modelGru1(seq1.float().to(device))\n",
    "        out, h2 = modelGru2(seq2.float().to(device))\n",
    "        h1, h2 = h1.squeeze(0), h2.squeeze(0)\n",
    "\n",
    "        # concatenate the hidden output\n",
    "        h = torch.cat((h1, h2), dim=1).to(device)\n",
    "\n",
    "        # Forward pass through the ANN (which will be 256 inputs)\n",
    "        y_pred = modelAnn(h).squeeze().to(device)\n",
    "        # Compute the loss\n",
    "        loss = criterion(y_pred, label.float().squeeze().to(device))\n",
    "        train_total_loss += loss.item()\n",
    "        # ipdb.set_trace() # y_pred.shape: [10000], label.shape: [10000]\n",
    "        #if y_pred.shape != label.shape:\n",
    "          #ipdb.set_trace()\n",
    "        # train_total_error += get_abs_percent_error(y_pred, label.float())\n",
    "        train_total_error += get_smape(y_pred, label.float().to(device))\n",
    " \n",
    "        # Zero the gradients and perform backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # I am going to only track one of them for ease here\n",
    "        # print(\"         True Label: \" + str(label) + \" Predicted Output: \" + str(y_pred))\n",
    "      \n",
    "      training_losses[epoch] = train_total_loss/num_train_batches # /\n",
    "      training_SMAPE[epoch] = train_total_error/num_train_batches\n",
    "\n",
    "\n",
    "      # VALIDATION LOOP\n",
    "      for seq1, seq2, label in validation_set:\n",
    "        num_val_batches += 1\n",
    "        # Forward pass through the two GRU (applied to the first and second sequency)\n",
    "        out, h1 = modelGru1(seq1.float().to(device))\n",
    "        out, h2 = modelGru2(seq2.float().to(device))\n",
    "        h1, h2 = h1.squeeze(0), h2.squeeze(0)\n",
    "\n",
    "        # concatenate the hidden output\n",
    "        h = torch.cat((h1, h2), dim=1).to(device)\n",
    "\n",
    "        # Forward pass through the ANN (which will be 256 inputs)\n",
    "        y_pred = modelAnn(h).squeeze().to(device)\n",
    "        # Compute the loss\n",
    "        loss = criterion(y_pred, label.float().squeeze().to(device))\n",
    "        val_total_loss += loss.item()\n",
    "        #ipdb.set_trace()\n",
    "        val_total_error += get_smape(y_pred, label.float().to(device))\n",
    "      \n",
    "      validation_losses[epoch] = val_total_loss/num_val_batches\n",
    "      validation_SMAPE[epoch] = val_total_error/num_val_batches\n",
    "      \n",
    "      print(f\"Epoch {epoch} | Current Training Loss: {training_losses[epoch]} | Error: {training_SMAPE[epoch] * 100.0}%\")\n",
    "      print(f\"Epoch {epoch} | Current Validation Loss: {validation_losses[epoch]} | Error: {validation_SMAPE[epoch] * 100.0}%\")\n",
    "      file_data.write(f\"Epoch {epoch} | Current Training Loss: {training_losses[epoch]} | Error: {training_SMAPE[epoch] * 100.0}%\\n\")\n",
    "      file_data.write(f\"Epoch {epoch} | Current Validation Loss: {validation_losses[epoch]} | Error: {validation_SMAPE[epoch] * 100.0}%\\n\")\n",
    "      if epoch in [10, 50, 100]:\n",
    "        modelGru1_path = f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}\"\n",
    "        torch.save(modelGru1.state_dict(), modelGru1_path)\n",
    "        modelGru2_path = f\"{path}/3bp_{date_time}_model_Gru2_hiddensz{modelGru2.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}\"\n",
    "        torch.save(modelGru2.state_dict(), modelGru2_path)       \n",
    "        modelAnn_path = f\"{path}/3bp_{date_time}_model_Ann_hiddensz{modelGru2.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}\" \n",
    "        torch.save(modelAnn.state_dict(), modelAnn_path)\n",
    "        \n",
    "        plot_losses(iterations, training_losses, validation_losses, f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}_loss.png\")\n",
    "        plot_SMAPE(iterations, training_SMAPE, validation_SMAPE, f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}_SMAPE.png\")\n",
    "\n",
    "   \n",
    "    file_data.close()\n",
    "    # Save the current model (checkpoint) to a file\n",
    "    \n",
    "    modelGru1_path = f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}\"\n",
    "    torch.save(modelGru1.state_dict(), modelGru1_path)\n",
    "    modelGru2_path = f\"{path}/3bp_{date_time}_model_Gru2_hiddensz{modelGru2.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}\"\n",
    "    torch.save(modelGru2.state_dict(), modelGru2_path)       \n",
    "    modelAnn_path = f\"{path}/3bp_{date_time}_model_Ann_hiddensz{modelGru2.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}\" \n",
    "    torch.save(modelAnn.state_dict(), modelAnn_path)\n",
    "    \n",
    "    plot_losses(iterations, training_losses, validation_losses, f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}_loss.png\")\n",
    "    plot_SMAPE(iterations, training_SMAPE, validation_SMAPE, f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}_SMAPE.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "S4bwMcvM6TwD"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)  # initial hidden state\n",
    "        out, hn = self.gru(x)  # out shape: (batch_size, sequence_length, hidden_size), hn shape: (1, batch_size, hidden_size)\n",
    "        return out, hn\n",
    "        #return out[:, -1, :]  # return the last hidden state, shape: (batch_size, hidden_size)\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 20)  # 1st hidden layer with 64 nodes\n",
    "        self.fc2 = nn.Linear(20, 10)  # 1st hidden layer with 64 nodes\n",
    "        self.output_layer = nn.Linear(10, 1)  # Output layer with 1 node\n",
    "        self.p = 0\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))  # Apply ReLU activation to 1st hidden layer output\n",
    "        x = nn.functional.relu(self.fc2(x))  # Apply ReLU activation to 1st hidden layer output\n",
    "        output = self.output_layer(x)        # Output layer\n",
    "        return output\n",
    "\n",
    "class ANN_dropout(nn.Module):\n",
    "  def __init__(self, input_size, p):\n",
    "        super(ANN_dropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 20)  # 1st hidden layer with 64 nodes\n",
    "        self.fc2 = nn.Linear(20, 10)  # 1st hidden layer with 64 nodes\n",
    "        self.p = p\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.output_layer = nn.Linear(10, 1)  # Output layer with 1 node\n",
    "        \n",
    "  def forward(self, x):\n",
    "    x = nn.functional.relu(self.fc1(x))  # Apply ReLU activation to 1st hidden layer output\n",
    "    x = nn.functional.relu(self.fc2(x))  # Apply ReLU activation to 1st hidden layer output\n",
    "    x = self.dropout(x)\n",
    "    output = self.output_layer(x)  # Output layer\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "icLktHeG6L4V"
   },
   "outputs": [],
   "source": [
    "def normalized_data_pairings_log(list_pairs):\n",
    "    '''INPUT: A list of the data in full dataset, of the form:\n",
    "            [[\"AAA\", \"TTT\", num_binding_events0],\n",
    "                            ...\n",
    "             [\"AAC\", \"TTG\", num_binding_events1]]\n",
    "     OUTPUT: a normalized tuple array of the data (helps with comparision)'''\n",
    "    norm_list_pairs = []\n",
    "    # doing fancy log stuff\n",
    "    total_binding_events = np.log(np.array(list_pairs)[:,2].astype(int).sum())\n",
    "    for pair in list_pairs:\n",
    "      # doing fancy log stuff\n",
    "      affinity = float(np.log(pair[2] + 1)/total_binding_events) \n",
    "      assert type(affinity) == float\n",
    "      norm_list_pairs.append([pair[0], pair[1], affinity])\n",
    "    return norm_list_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYLs9aWd9Bke"
   },
   "source": [
    "Inputting Data and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yVkSVND92jOh"
   },
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqAvgban2vSD"
   },
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l3-va9qX2xcf"
   },
   "outputs": [],
   "source": [
    "learning_rates = [1.0e-5]\n",
    "num_epoch = 250\n",
    "# GRU params\n",
    "input_size = 2 # size of the encoding\n",
    "hidden_sizes = [15, 30, 50] # number of hidden states in the GRU\n",
    "# ANN params\n",
    "dropout = [0.1]\n",
    "batch_sizes = [1, 5, 10] # batches in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYeLY4xj2y-T",
    "outputId": "0f805eee-1065-4c77-d5f3-4621461e30ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose learning rate 1e-05, number of epochs 2, hidden_size 15, batch size 1, dropout 0.1\n",
      "Epoch 0 | Current Training Loss: 0.021812733715467617 | Error: 107.94802904129028%\n",
      "Epoch 0 | Current Validation Loss: 0.014450430140124017 | Error: 159.882915019989%\n",
      "Epoch 1 | Current Training Loss: 0.021535595880022303 | Error: 106.44769668579102%\n",
      "Epoch 1 | Current Validation Loss: 0.014350013939918974 | Error: 160.10401248931885%\n",
      "Epoch 0 | Current Training Loss: 0.0290190809028469 | Error: 115.57183265686035%\n",
      "Epoch 0 | Current Validation Loss: 0.01693304132716885 | Error: 159.52410697937012%\n"
     ]
    }
   ],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    i = 0\n",
    "    # create imbeddings \n",
    "    embedded = convert_alpha_sequence(normalized_data_pairings_log(data_pairings))\n",
    "\n",
    "    # create data loader, we will train on the same one for consistency\n",
    "    train_loader, val_loader, test_loader = batching_fcn(dataset = embedded, split_proportion = [0.80, 0.18, 0.02], batch_size = batch_size, split_seed=30, batching_seed=42, balance = True)\n",
    "    for dropout_p in dropout:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for learning_rate in learning_rates:\n",
    "                print(f\"Chose learning rate {learning_rate}, number of epochs {num_epoch}, hidden_size {hidden_size}, batch size {batch_size}, dropout {dropout_p}\")\n",
    "                # creating the structures required for model\n",
    "                gru1 = GRU(input_size, hidden_size).to('cuda')\n",
    "                gru2 = GRU(input_size, hidden_size).to('cuda')\n",
    "                ann = ANN_dropout(hidden_size*2, dropout_p).to('cuda')\n",
    "                ann_noD = ANN(hidden_size*2).to('cuda')\n",
    "                # train\n",
    "                train(gru1, gru2, ann, train_loader, val_loader, batch_size=batch_size, num_epochs=num_epoch, learning_rate=learning_rate, plot=True, path=f\"./series{i:02d}\")\n",
    "                train(gru1, gru2, ann_noD, train_loader, val_loader, batch_size=batch_size, num_epochs=num_epoch, learning_rate=learning_rate, plot=True, path=f\"./series{i:02d}_nodropout\")\n",
    "    i += 1        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbLgXAKo25ux"
   },
   "outputs": [],
   "source": [
    "learning_rates = [1.0e-7]\n",
    "num_epoch = 2\n",
    "# GRU params\n",
    "input_size = 2 # size of the encoding\n",
    "hidden_sizes = [15, 30, 50] # number of hidden states in the GRU\n",
    "# ANN params\n",
    "dropout = [0.1]\n",
    "batch_sizes = [1, 5, 10] # batches in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2UcpcI92-nS"
   },
   "outputs": [],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    i = 100\n",
    "    # create imbeddings \n",
    "    embedded = convert_alpha_sequence(normalized_data_pairings_log(data_pairings))\n",
    "    embedded = embedded.to('cuda')\n",
    "    # create data loader, we will train on the same one for consistency\n",
    "    train_loader, val_loader, test_loader = batching_fcn(dataset = embedded, split_proportion = [0.80, 0.18, 0.02], batch_size = batch_size, split_seed=30, batching_seed=42, balance = True)\n",
    "    for dropout_p in dropout:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for learning_rate in learning_rates:\n",
    "                print(f\"Chose learning rate {learning_rate}, number of epochs {num_epoch}, hidden_size {hidden_size}, batch size {batch_size}, dropout {dropout_p}\")\n",
    "                # creating the structures required for model\n",
    "                gru1 = GRU(input_size, hidden_size).to('cuda')\n",
    "                gru2 = GRU(input_size, hidden_size).to('cuda')\n",
    "                ann = ANN_dropout(hidden_size*2, dropout_p).to('cuda')\n",
    "                ann_noD = ANN(hidden_size*2).to('cuda')\n",
    "                # train\n",
    "                train(gru1, gru2, ann, train_loader, val_loader, batch_size=batch_size, num_epochs=num_epoch, learning_rate=learning_rate, plot=True, path=f\"./series{i:02d}\")\n",
    "                train(gru1, gru2, ann_noD, train_loader, val_loader, batch_size=batch_size, num_epochs=num_epoch, learning_rate=learning_rate, plot=True, path=f\"./series{i:02d}_nodropout\")\n",
    "    i += 1        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-H86hD43AUZ"
   },
   "outputs": [],
   "source": [
    "learning_rates = [1.0e-5]\n",
    "num_epoch = 2\n",
    "# GRU params\n",
    "input_size = 2 # size of the encoding\n",
    "hidden_sizes = [15, 30, 50] # number of hidden states in the GRU\n",
    "# ANN params\n",
    "dropout = [0.3]\n",
    "batch_sizes = [1, 5, 10] # batches in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swcTLCEh3DfZ"
   },
   "outputs": [],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    i = 200\n",
    "    # create imbeddings \n",
    "    embedded = convert_alpha_sequence(normalized_data_pairings_log(data_pairings))\n",
    "    embedded = embedded.to('cuda')\n",
    "    # create data loader, we will train on the same one for consistency\n",
    "    train_loader, val_loader, test_loader = batching_fcn(dataset = embedded, split_proportion = [0.80, 0.18, 0.02], batch_size = batch_size, split_seed=30, batching_seed=42, balance = True)\n",
    "    for dropout_p in dropout:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for learning_rate in learning_rates:\n",
    "                print(f\"Chose learning rate {learning_rate}, number of epochs {num_epoch}, hidden_size {hidden_size}, batch size {batch_size}, dropout {dropout_p}\")\n",
    "                # creating the structures required for model\n",
    "                gru1 = GRU(input_size, hidden_size).to('cuda')\n",
    "                gru2 = GRU(input_size, hidden_size).to('cuda')\n",
    "                ann = ANN_dropout(hidden_size*2, dropout_p).to('cuda')\n",
    "                ann_noD = ANN(hidden_size*2).to('cuda')\n",
    "                # train\n",
    "                train(gru1, gru2, ann, train_loader, val_loader, batch_size=batch_size, num_epochs=num_epoch, learning_rate=learning_rate, plot=True, path=f\"./series{i:02d}\")\n",
    "                train(gru1, gru2, ann_noD, train_loader, val_loader, batch_size=batch_size, num_epochs=num_epoch, learning_rate=learning_rate, plot=True, path=f\"./series{i:02d}_nodropout\")\n",
    "    i += 1        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ytBRO6N93GVD"
   },
   "outputs": [],
   "source": [
    "learning_rates = [1.0e-7]\n",
    "num_epoch = 250\n",
    "# GRU params\n",
    "input_size = 2 # size of the encoding\n",
    "hidden_sizes = [15, 30, 50] # number of hidden states in the GRU\n",
    "# ANN params\n",
    "dropout = [0.3]\n",
    "batch_sizes = [1, 5, 10] # batches in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x6DDWU3N3HAn",
    "outputId": "d725fdcd-8a52-4923-eaeb-df3f3d67348c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose learning rate 1e-07, number of epochs 250, hidden_size 15, batch size 1, dropout 0.3\n",
      "Epoch 0 | Current Training Loss: 0.1707312593638876 | Error: 200.0%\n",
      "Epoch 0 | Current Validation Loss: 0.11601466866237904 | Error: 200.0%\n",
      "Epoch 1 | Current Training Loss: 0.16745493034525222 | Error: 200.0%\n",
      "Epoch 1 | Current Validation Loss: 0.11476985393711234 | Error: 200.0%\n",
      "Epoch 2 | Current Training Loss: 0.16357335515586766 | Error: 200.0%\n",
      "Epoch 2 | Current Validation Loss: 0.11404698985291385 | Error: 200.0%\n",
      "Epoch 3 | Current Training Loss: 0.1605861640515702 | Error: 200.0%\n",
      "Epoch 3 | Current Validation Loss: 0.11241188875602981 | Error: 200.0%\n",
      "Epoch 4 | Current Training Loss: 0.15700128881252914 | Error: 200.0%\n",
      "Epoch 4 | Current Validation Loss: 0.10699344321509148 | Error: 200.0%\n",
      "Epoch 5 | Current Training Loss: 0.1549332793893457 | Error: 200.0%\n",
      "Epoch 5 | Current Validation Loss: 0.10292482951067115 | Error: 200.0%\n",
      "Epoch 6 | Current Training Loss: 0.15190351855788292 | Error: 200.0%\n",
      "Epoch 6 | Current Validation Loss: 0.10107001688791341 | Error: 200.0%\n",
      "Epoch 7 | Current Training Loss: 0.1482473634159895 | Error: 200.0%\n",
      "Epoch 7 | Current Validation Loss: 0.09905709519102826 | Error: 200.0%\n",
      "Epoch 8 | Current Training Loss: 0.1455031140235894 | Error: 200.0%\n",
      "Epoch 8 | Current Validation Loss: 0.09655375242627541 | Error: 200.0%\n",
      "Epoch 9 | Current Training Loss: 0.1429767132753723 | Error: 200.0%\n",
      "Epoch 9 | Current Validation Loss: 0.09498676708981577 | Error: 200.0%\n",
      "Epoch 10 | Current Training Loss: 0.14004024882653276 | Error: 200.0%\n",
      "Epoch 10 | Current Validation Loss: 0.09119106051000278 | Error: 200.0%\n",
      "Epoch 11 | Current Training Loss: 0.13706498535619044 | Error: 200.0%\n",
      "Epoch 11 | Current Validation Loss: 0.0895353308556604 | Error: 200.0%\n",
      "Epoch 12 | Current Training Loss: 0.1338900010583486 | Error: 200.0%\n",
      "Epoch 12 | Current Validation Loss: 0.08630391366380485 | Error: 200.0%\n",
      "Epoch 13 | Current Training Loss: 0.13207764279366396 | Error: 200.0%\n",
      "Epoch 13 | Current Validation Loss: 0.08475142530591469 | Error: 200.0%\n",
      "Epoch 14 | Current Training Loss: 0.12915516995092094 | Error: 200.0%\n",
      "Epoch 14 | Current Validation Loss: 0.08345550079065472 | Error: 200.0%\n",
      "Epoch 15 | Current Training Loss: 0.12602726273123713 | Error: 200.0%\n",
      "Epoch 15 | Current Validation Loss: 0.08150347460976207 | Error: 200.0%\n",
      "Epoch 16 | Current Training Loss: 0.1238082971660287 | Error: 200.0%\n",
      "Epoch 16 | Current Validation Loss: 0.07910798524798297 | Error: 200.0%\n",
      "Epoch 17 | Current Training Loss: 0.12141210867253982 | Error: 200.0%\n",
      "Epoch 17 | Current Validation Loss: 0.07502853729240243 | Error: 200.0%\n",
      "Epoch 18 | Current Training Loss: 0.11906230406950621 | Error: 200.0%\n",
      "Epoch 18 | Current Validation Loss: 0.07504350004598923 | Error: 200.0%\n",
      "Epoch 19 | Current Training Loss: 0.11590091165580994 | Error: 200.0%\n",
      "Epoch 19 | Current Validation Loss: 0.07188071509926824 | Error: 200.0%\n",
      "Epoch 20 | Current Training Loss: 0.11339702518032485 | Error: 200.0%\n",
      "Epoch 20 | Current Validation Loss: 0.07138894080706294 | Error: 200.0%\n",
      "Epoch 21 | Current Training Loss: 0.11151289123124841 | Error: 200.0%\n",
      "Epoch 21 | Current Validation Loss: 0.06798026716814275 | Error: 200.0%\n",
      "Epoch 22 | Current Training Loss: 0.10847976552355162 | Error: 200.0%\n",
      "Epoch 22 | Current Validation Loss: 0.06611185098696232 | Error: 200.0%\n",
      "Epoch 23 | Current Training Loss: 0.10573827598680223 | Error: 200.0%\n",
      "Epoch 23 | Current Validation Loss: 0.06585798961371993 | Error: 200.0%\n",
      "Epoch 24 | Current Training Loss: 0.1040046701606802 | Error: 200.0%\n",
      "Epoch 24 | Current Validation Loss: 0.06143874101204295 | Error: 200.0%\n",
      "Epoch 25 | Current Training Loss: 0.10190067804159876 | Error: 200.0%\n",
      "Epoch 25 | Current Validation Loss: 0.06143432853584212 | Error: 200.0%\n",
      "Epoch 26 | Current Training Loss: 0.09971019322285102 | Error: 200.0%\n",
      "Epoch 26 | Current Validation Loss: 0.06080651354345429 | Error: 200.0%\n",
      "Epoch 27 | Current Training Loss: 0.09699278101031139 | Error: 200.0%\n",
      "Epoch 27 | Current Validation Loss: 0.057792826749642194 | Error: 200.0%\n",
      "Epoch 28 | Current Training Loss: 0.095765257951298 | Error: 200.0%\n",
      "Epoch 28 | Current Validation Loss: 0.05665839392011998 | Error: 200.0%\n",
      "Epoch 29 | Current Training Loss: 0.09356077430155232 | Error: 200.0%\n",
      "Epoch 29 | Current Validation Loss: 0.05435575548088079 | Error: 200.0%\n",
      "Epoch 30 | Current Training Loss: 0.0908670672204092 | Error: 200.0%\n",
      "Epoch 30 | Current Validation Loss: 0.05143434055474318 | Error: 200.0%\n",
      "Epoch 31 | Current Training Loss: 0.0892905843031735 | Error: 200.0%\n",
      "Epoch 31 | Current Validation Loss: 0.051049639279113795 | Error: 200.0%\n",
      "Epoch 32 | Current Training Loss: 0.08716266807273218 | Error: 200.0%\n",
      "Epoch 32 | Current Validation Loss: 0.04920913575133726 | Error: 200.0%\n",
      "Epoch 33 | Current Training Loss: 0.08476697667723922 | Error: 200.0%\n",
      "Epoch 33 | Current Validation Loss: 0.047861941137921006 | Error: 200.0%\n",
      "Epoch 34 | Current Training Loss: 0.08296193627788859 | Error: 200.0%\n",
      "Epoch 34 | Current Validation Loss: 0.0470474936948518 | Error: 200.0%\n",
      "Epoch 35 | Current Training Loss: 0.08128707001596717 | Error: 200.0%\n",
      "Epoch 35 | Current Validation Loss: 0.044999035250324775 | Error: 200.0%\n",
      "Epoch 36 | Current Training Loss: 0.07987561309973018 | Error: 200.0%\n",
      "Epoch 36 | Current Validation Loss: 0.04488519412968333 | Error: 200.0%\n",
      "Epoch 37 | Current Training Loss: 0.07752705672058652 | Error: 200.0%\n",
      "Epoch 37 | Current Validation Loss: 0.04380745628486631 | Error: 200.0%\n",
      "Epoch 38 | Current Training Loss: 0.07626506374583307 | Error: 200.0%\n",
      "Epoch 38 | Current Validation Loss: 0.04216737670893749 | Error: 200.0%\n",
      "Epoch 39 | Current Training Loss: 0.0744714909533906 | Error: 200.0%\n",
      "Epoch 39 | Current Validation Loss: 0.040027212136658226 | Error: 200.0%\n",
      "Epoch 40 | Current Training Loss: 0.07315592808062948 | Error: 200.0%\n",
      "Epoch 40 | Current Validation Loss: 0.03978538604333474 | Error: 200.0%\n",
      "Epoch 41 | Current Training Loss: 0.0710380235831152 | Error: 200.0%\n",
      "Epoch 41 | Current Validation Loss: 0.03718752554314384 | Error: 200.0%\n",
      "Epoch 42 | Current Training Loss: 0.07070183710636253 | Error: 200.0%\n",
      "Epoch 42 | Current Validation Loss: 0.039218079008323255 | Error: 200.0%\n",
      "Epoch 43 | Current Training Loss: 0.06770300375678648 | Error: 200.0%\n",
      "Epoch 43 | Current Validation Loss: 0.0353319473547585 | Error: 200.0%\n",
      "Epoch 44 | Current Training Loss: 0.06783119091691808 | Error: 200.0%\n",
      "Epoch 44 | Current Validation Loss: 0.03664240728567094 | Error: 200.0%\n",
      "Epoch 45 | Current Training Loss: 0.06586590631839769 | Error: 199.9839186668396%\n",
      "Epoch 45 | Current Validation Loss: 0.033200299637037134 | Error: 199.9574065208435%\n",
      "Epoch 46 | Current Training Loss: 0.06474989104984814 | Error: 199.8957872390747%\n",
      "Epoch 46 | Current Validation Loss: 0.034653160745783486 | Error: 199.93197917938232%\n",
      "Epoch 47 | Current Training Loss: 0.06336550561060776 | Error: 199.55450296401978%\n",
      "Epoch 47 | Current Validation Loss: 0.03406646028348682 | Error: 199.79689121246338%\n",
      "Epoch 48 | Current Training Loss: 0.06254159979596512 | Error: 198.89131784439087%\n",
      "Epoch 48 | Current Validation Loss: 0.03173862905995139 | Error: 199.47339296340942%\n",
      "Epoch 49 | Current Training Loss: 0.06172146993026181 | Error: 198.23673963546753%\n",
      "Epoch 49 | Current Validation Loss: 0.03330938065227049 | Error: 199.18266534805298%\n",
      "Epoch 50 | Current Training Loss: 0.06067948905234419 | Error: 197.08975553512573%\n",
      "Epoch 50 | Current Validation Loss: 0.030516098885978605 | Error: 198.59142303466797%\n",
      "Epoch 51 | Current Training Loss: 0.0596784332872718 | Error: 196.0599422454834%\n",
      "Epoch 51 | Current Validation Loss: 0.03140160803769449 | Error: 197.53072261810303%\n",
      "Epoch 52 | Current Training Loss: 0.05842147364497756 | Error: 194.20803785324097%\n",
      "Epoch 52 | Current Validation Loss: 0.030018730284415245 | Error: 197.30666875839233%\n",
      "Epoch 53 | Current Training Loss: 0.057203334212729946 | Error: 192.43674278259277%\n",
      "Epoch 53 | Current Validation Loss: 0.029713749924336744 | Error: 197.18165397644043%\n",
      "Epoch 54 | Current Training Loss: 0.057037405719096296 | Error: 189.97271060943604%\n",
      "Epoch 54 | Current Validation Loss: 0.02943938044132763 | Error: 195.1485276222229%\n",
      "Epoch 55 | Current Training Loss: 0.05542855742490652 | Error: 187.74832487106323%\n",
      "Epoch 55 | Current Validation Loss: 0.027353191611446497 | Error: 195.20492553710938%\n",
      "Epoch 56 | Current Training Loss: 0.05441084440143625 | Error: 185.3865146636963%\n",
      "Epoch 56 | Current Validation Loss: 0.025655470026933955 | Error: 192.40095615386963%\n",
      "Epoch 57 | Current Training Loss: 0.05395469833561774 | Error: 183.58396291732788%\n",
      "Epoch 57 | Current Validation Loss: 0.02899315835938801 | Error: 193.5417890548706%\n",
      "Epoch 58 | Current Training Loss: 0.053624569710332526 | Error: 181.19999170303345%\n",
      "Epoch 58 | Current Validation Loss: 0.026998896051198543 | Error: 191.10119342803955%\n",
      "Epoch 59 | Current Training Loss: 0.05177423644470462 | Error: 178.5662055015564%\n",
      "Epoch 59 | Current Validation Loss: 0.02714015986640785 | Error: 191.81605577468872%\n",
      "Epoch 60 | Current Training Loss: 0.050770443631523506 | Error: 177.20248699188232%\n",
      "Epoch 60 | Current Validation Loss: 0.0257538738886804 | Error: 191.18043184280396%\n",
      "Epoch 61 | Current Training Loss: 0.05042419457793972 | Error: 175.67037343978882%\n",
      "Epoch 61 | Current Validation Loss: 0.027116835814569973 | Error: 190.8084750175476%\n",
      "Epoch 62 | Current Training Loss: 0.04952223642804945 | Error: 174.31377172470093%\n",
      "Epoch 62 | Current Validation Loss: 0.025118222837019036 | Error: 190.83949327468872%\n",
      "Epoch 63 | Current Training Loss: 0.049615173637171654 | Error: 172.1362829208374%\n",
      "Epoch 63 | Current Validation Loss: 0.02501308130717639 | Error: 188.71668577194214%\n",
      "Epoch 64 | Current Training Loss: 0.04900532976966342 | Error: 172.38292694091797%\n",
      "Epoch 64 | Current Validation Loss: 0.026433972208924093 | Error: 190.25756120681763%\n",
      "Epoch 65 | Current Training Loss: 0.046959756305974404 | Error: 170.22340297698975%\n",
      "Epoch 65 | Current Validation Loss: 0.023686594272496884 | Error: 187.7100110054016%\n",
      "Epoch 66 | Current Training Loss: 0.04678084778759473 | Error: 171.15283012390137%\n",
      "Epoch 66 | Current Validation Loss: 0.024760229243133584 | Error: 188.77733945846558%\n",
      "Epoch 67 | Current Training Loss: 0.046066381833981936 | Error: 169.5928931236267%\n",
      "Epoch 67 | Current Validation Loss: 0.02421764273457311 | Error: 188.69794607162476%\n",
      "Epoch 68 | Current Training Loss: 0.04555582544625368 | Error: 168.2368278503418%\n",
      "Epoch 68 | Current Validation Loss: 0.023265817434106776 | Error: 186.26649379730225%\n",
      "Epoch 69 | Current Training Loss: 0.044764333055410076 | Error: 167.48822927474976%\n",
      "Epoch 69 | Current Validation Loss: 0.02472805225159237 | Error: 188.99502754211426%\n",
      "Epoch 70 | Current Training Loss: 0.04481423336016408 | Error: 168.29296350479126%\n",
      "Epoch 70 | Current Validation Loss: 0.02333032039525036 | Error: 185.9461545944214%\n",
      "Epoch 71 | Current Training Loss: 0.04416358934329207 | Error: 167.56558418273926%\n",
      "Epoch 71 | Current Validation Loss: 0.021222959315244604 | Error: 183.82678031921387%\n",
      "Epoch 72 | Current Training Loss: 0.04452971897765651 | Error: 166.43387079238892%\n",
      "Epoch 72 | Current Validation Loss: 0.023403136559971442 | Error: 188.08363676071167%\n",
      "Epoch 73 | Current Training Loss: 0.04326994548662852 | Error: 165.49431085586548%\n",
      "Epoch 73 | Current Validation Loss: 0.02097369197008673 | Error: 185.35335063934326%\n",
      "Epoch 74 | Current Training Loss: 0.043608006562748755 | Error: 165.83911180496216%\n",
      "Epoch 74 | Current Validation Loss: 0.022744491052969707 | Error: 187.53135204315186%\n",
      "Epoch 75 | Current Training Loss: 0.042011696162027864 | Error: 165.22058248519897%\n",
      "Epoch 75 | Current Validation Loss: 0.022692261713853013 | Error: 185.4731559753418%\n",
      "Epoch 76 | Current Training Loss: 0.04210988552279008 | Error: 164.64828252792358%\n",
      "Epoch 76 | Current Validation Loss: 0.02176717555403356 | Error: 181.83627128601074%\n",
      "Epoch 77 | Current Training Loss: 0.041701204189177214 | Error: 165.1427984237671%\n",
      "Epoch 77 | Current Validation Loss: 0.020898659058704628 | Error: 184.96785163879395%\n",
      "Epoch 78 | Current Training Loss: 0.041848546429751214 | Error: 164.08578157424927%\n",
      "Epoch 78 | Current Validation Loss: 0.020615805092537156 | Error: 182.8895092010498%\n",
      "Epoch 79 | Current Training Loss: 0.04093655610701872 | Error: 163.87672424316406%\n",
      "Epoch 79 | Current Validation Loss: 0.02045393312281613 | Error: 183.9207410812378%\n",
      "Epoch 80 | Current Training Loss: 0.04031552212264968 | Error: 162.63527870178223%\n",
      "Epoch 80 | Current Validation Loss: 0.020203233385824325 | Error: 183.76586437225342%\n",
      "Epoch 81 | Current Training Loss: 0.040150076155434325 | Error: 162.52508163452148%\n",
      "Epoch 81 | Current Validation Loss: 0.021681123817337892 | Error: 187.0034098625183%\n",
      "Epoch 82 | Current Training Loss: 0.040063741222185985 | Error: 161.85559034347534%\n",
      "Epoch 82 | Current Validation Loss: 0.021618079185710407 | Error: 184.57155227661133%\n",
      "Epoch 83 | Current Training Loss: 0.03973580304275545 | Error: 161.84780597686768%\n",
      "Epoch 83 | Current Validation Loss: 0.02162472468889049 | Error: 184.23908948898315%\n",
      "Epoch 84 | Current Training Loss: 0.03967555514034136 | Error: 160.86913347244263%\n",
      "Epoch 84 | Current Validation Loss: 0.021521598065525498 | Error: 183.41494798660278%\n",
      "Epoch 85 | Current Training Loss: 0.039037603756501706 | Error: 158.17975997924805%\n",
      "Epoch 85 | Current Validation Loss: 0.02181972922503698 | Error: 182.70208835601807%\n",
      "Epoch 86 | Current Training Loss: 0.03813495813694262 | Error: 158.06154012680054%\n",
      "Epoch 86 | Current Validation Loss: 0.02151330786676976 | Error: 182.16819763183594%\n",
      "Epoch 87 | Current Training Loss: 0.03859859271373279 | Error: 157.87314176559448%\n",
      "Epoch 87 | Current Validation Loss: 0.021900104318506298 | Error: 181.15066289901733%\n",
      "Epoch 88 | Current Training Loss: 0.03802433543962986 | Error: 156.7397117614746%\n",
      "Epoch 88 | Current Validation Loss: 0.020775329055444044 | Error: 182.2643756866455%\n",
      "Epoch 89 | Current Training Loss: 0.0386084316759207 | Error: 158.2266330718994%\n",
      "Epoch 89 | Current Validation Loss: 0.019108055546089825 | Error: 181.36767148971558%\n",
      "Epoch 90 | Current Training Loss: 0.038598039857221135 | Error: 155.26472330093384%\n",
      "Epoch 90 | Current Validation Loss: 0.0210737665810511 | Error: 178.2266139984131%\n",
      "Epoch 91 | Current Training Loss: 0.037894737725583404 | Error: 154.55660820007324%\n",
      "Epoch 91 | Current Validation Loss: 0.020590154411404847 | Error: 180.31595945358276%\n",
      "Epoch 92 | Current Training Loss: 0.03743548751621192 | Error: 154.00739908218384%\n",
      "Epoch 92 | Current Validation Loss: 0.022072378950525263 | Error: 180.6788682937622%\n",
      "Epoch 93 | Current Training Loss: 0.038115906016801926 | Error: 155.29941320419312%\n",
      "Epoch 93 | Current Validation Loss: 0.0209970547888248 | Error: 180.68798780441284%\n",
      "Epoch 94 | Current Training Loss: 0.0375589878220141 | Error: 153.01650762557983%\n",
      "Epoch 94 | Current Validation Loss: 0.02135333001082801 | Error: 178.25884819030762%\n",
      "Epoch 95 | Current Training Loss: 0.03731496108582706 | Error: 152.46039628982544%\n",
      "Epoch 95 | Current Validation Loss: 0.02098476258931776 | Error: 178.80511283874512%\n",
      "Epoch 96 | Current Training Loss: 0.038352269138679874 | Error: 152.25261449813843%\n",
      "Epoch 96 | Current Validation Loss: 0.022453471605132815 | Error: 180.6440234184265%\n",
      "Epoch 97 | Current Training Loss: 0.0379341074614015 | Error: 151.36598348617554%\n",
      "Epoch 97 | Current Validation Loss: 0.020676727033039653 | Error: 177.53769159317017%\n",
      "Epoch 98 | Current Training Loss: 0.03716139052816415 | Error: 150.799560546875%\n",
      "Epoch 98 | Current Validation Loss: 0.020832718228958425 | Error: 180.53067922592163%\n",
      "Epoch 99 | Current Training Loss: 0.036899109833950755 | Error: 149.98317956924438%\n",
      "Epoch 99 | Current Validation Loss: 0.022317390019173648 | Error: 177.0031452178955%\n",
      "Epoch 100 | Current Training Loss: 0.03720008885774856 | Error: 149.1866111755371%\n",
      "Epoch 100 | Current Validation Loss: 0.021794801152686564 | Error: 178.34008932113647%\n",
      "Epoch 101 | Current Training Loss: 0.036835280523580136 | Error: 149.21079874038696%\n",
      "Epoch 101 | Current Validation Loss: 0.022650590993788078 | Error: 178.8213014602661%\n",
      "Epoch 102 | Current Training Loss: 0.03699060104378544 | Error: 147.81898260116577%\n",
      "Epoch 102 | Current Validation Loss: 0.0208783967970149 | Error: 177.72008180618286%\n",
      "Epoch 103 | Current Training Loss: 0.03632749273121705 | Error: 147.72478342056274%\n",
      "Epoch 103 | Current Validation Loss: 0.02223206155158148 | Error: 178.91335487365723%\n",
      "Epoch 104 | Current Training Loss: 0.03680185139406453 | Error: 147.64763116836548%\n",
      "Epoch 104 | Current Validation Loss: 0.021296536896012873 | Error: 176.84011459350586%\n",
      "Epoch 105 | Current Training Loss: 0.03689521084831819 | Error: 147.56689071655273%\n",
      "Epoch 105 | Current Validation Loss: 0.021692463189340528 | Error: 178.15852165222168%\n",
      "Epoch 106 | Current Training Loss: 0.03674389349177393 | Error: 146.01659774780273%\n",
      "Epoch 106 | Current Validation Loss: 0.02358477195956266 | Error: 178.10161113739014%\n",
      "Epoch 107 | Current Training Loss: 0.036702157079734045 | Error: 146.02298736572266%\n",
      "Epoch 107 | Current Validation Loss: 0.021473711573078015 | Error: 175.67026615142822%\n",
      "Epoch 108 | Current Training Loss: 0.036334035459664654 | Error: 145.03813982009888%\n",
      "Epoch 108 | Current Validation Loss: 0.021956511058803287 | Error: 174.93245601654053%\n",
      "Epoch 109 | Current Training Loss: 0.03598898271729599 | Error: 144.4362759590149%\n",
      "Epoch 109 | Current Validation Loss: 0.020495722166555753 | Error: 174.20694828033447%\n",
      "Epoch 110 | Current Training Loss: 0.036709434575480496 | Error: 144.9453353881836%\n",
      "Epoch 110 | Current Validation Loss: 0.02204394013647112 | Error: 174.713134765625%\n",
      "Epoch 111 | Current Training Loss: 0.03636843562293034 | Error: 143.45285892486572%\n",
      "Epoch 111 | Current Validation Loss: 0.021155668359912852 | Error: 175.83884000778198%\n",
      "Epoch 112 | Current Training Loss: 0.03614659741558394 | Error: 142.9180145263672%\n",
      "Epoch 112 | Current Validation Loss: 0.0211479744648363 | Error: 176.8460988998413%\n",
      "Epoch 113 | Current Training Loss: 0.03599671902625688 | Error: 142.64823198318481%\n",
      "Epoch 113 | Current Validation Loss: 0.02382680214394631 | Error: 176.30069255828857%\n",
      "Epoch 114 | Current Training Loss: 0.03580889535962646 | Error: 142.54387617111206%\n",
      "Epoch 114 | Current Validation Loss: 0.024415698569907492 | Error: 176.11031532287598%\n",
      "Epoch 115 | Current Training Loss: 0.03563868864511541 | Error: 142.4968123435974%\n",
      "Epoch 115 | Current Validation Loss: 0.022902472520269165 | Error: 176.515531539917%\n",
      "Epoch 116 | Current Training Loss: 0.0354788576907057 | Error: 141.93427562713623%\n",
      "Epoch 116 | Current Validation Loss: 0.022759836831505665 | Error: 174.7411847114563%\n",
      "Epoch 117 | Current Training Loss: 0.03576652230341721 | Error: 141.9607639312744%\n",
      "Epoch 117 | Current Validation Loss: 0.021797341391453794 | Error: 175.90100765228271%\n",
      "Epoch 118 | Current Training Loss: 0.035958856547116176 | Error: 142.2938585281372%\n",
      "Epoch 118 | Current Validation Loss: 0.022524750729348286 | Error: 177.4100661277771%\n",
      "Epoch 119 | Current Training Loss: 0.036219316277930985 | Error: 141.4060115814209%\n",
      "Epoch 119 | Current Validation Loss: 0.022841166487953375 | Error: 176.5389323234558%\n",
      "Epoch 120 | Current Training Loss: 0.03619041567602161 | Error: 141.002357006073%\n",
      "Epoch 120 | Current Validation Loss: 0.02314799251565129 | Error: 177.24816799163818%\n",
      "Epoch 121 | Current Training Loss: 0.036187834520387804 | Error: 142.0873522758484%\n",
      "Epoch 121 | Current Validation Loss: 0.024496398653435874 | Error: 175.73888301849365%\n",
      "Epoch 122 | Current Training Loss: 0.0356865959526342 | Error: 139.9117350578308%\n",
      "Epoch 122 | Current Validation Loss: 0.022154278189874373 | Error: 173.83267879486084%\n",
      "Epoch 123 | Current Training Loss: 0.035351146596011865 | Error: 140.73805809020996%\n",
      "Epoch 123 | Current Validation Loss: 0.023006480683580376 | Error: 174.01620149612427%\n",
      "Epoch 124 | Current Training Loss: 0.03542557396565214 | Error: 140.119469165802%\n",
      "Epoch 124 | Current Validation Loss: 0.022142522365889022 | Error: 174.37280416488647%\n",
      "Epoch 125 | Current Training Loss: 0.03627780083856716 | Error: 140.79911708831787%\n",
      "Epoch 125 | Current Validation Loss: 0.022332890549864686 | Error: 174.80897903442383%\n",
      "Epoch 126 | Current Training Loss: 0.03685556792673104 | Error: 140.001380443573%\n",
      "Epoch 126 | Current Validation Loss: 0.020794799691940764 | Error: 173.5878586769104%\n",
      "Epoch 127 | Current Training Loss: 0.035584772789520665 | Error: 139.16714191436768%\n",
      "Epoch 127 | Current Validation Loss: 0.023190687149620727 | Error: 175.50699710845947%\n",
      "Epoch 128 | Current Training Loss: 0.035563345002877084 | Error: 140.4871702194214%\n",
      "Epoch 128 | Current Validation Loss: 0.02234432357508624 | Error: 172.83565998077393%\n",
      "Epoch 129 | Current Training Loss: 0.03576358252522409 | Error: 139.31080102920532%\n",
      "Epoch 129 | Current Validation Loss: 0.021049629596704065 | Error: 172.10367918014526%\n",
      "Epoch 130 | Current Training Loss: 0.035448222747565615 | Error: 139.3919825553894%\n",
      "Epoch 130 | Current Validation Loss: 0.022373925416024322 | Error: 173.8016963005066%\n",
      "Epoch 131 | Current Training Loss: 0.03637374855025837 | Error: 139.70462083816528%\n",
      "Epoch 131 | Current Validation Loss: 0.023179562672545218 | Error: 175.00851154327393%\n",
      "Epoch 132 | Current Training Loss: 0.036143457990490464 | Error: 139.448082447052%\n",
      "Epoch 132 | Current Validation Loss: 0.022025905598096033 | Error: 172.6479172706604%\n",
      "Epoch 133 | Current Training Loss: 0.035975588540847495 | Error: 139.13480043411255%\n",
      "Epoch 133 | Current Validation Loss: 0.022658606659188608 | Error: 175.12763738632202%\n",
      "Epoch 134 | Current Training Loss: 0.03515757838424048 | Error: 138.258159160614%\n",
      "Epoch 134 | Current Validation Loss: 0.022426429990752048 | Error: 174.4471311569214%\n",
      "Epoch 135 | Current Training Loss: 0.03583697250076709 | Error: 138.14198970794678%\n",
      "Epoch 135 | Current Validation Loss: 0.02171223270870417 | Error: 170.50154209136963%\n",
      "Epoch 136 | Current Training Loss: 0.03480654748848649 | Error: 137.56111860275269%\n",
      "Epoch 136 | Current Validation Loss: 0.022397251340119207 | Error: 173.3033299446106%\n",
      "Epoch 137 | Current Training Loss: 0.035439714713497945 | Error: 138.62088918685913%\n",
      "Epoch 137 | Current Validation Loss: 0.022425173548407083 | Error: 174.8498797416687%\n",
      "Epoch 138 | Current Training Loss: 0.03570922691618004 | Error: 138.47671747207642%\n",
      "Epoch 138 | Current Validation Loss: 0.02219005980487309 | Error: 174.73642826080322%\n",
      "Epoch 139 | Current Training Loss: 0.035528330865246746 | Error: 137.71346807479858%\n",
      "Epoch 139 | Current Validation Loss: 0.021715474215022203 | Error: 174.4144082069397%\n",
      "Epoch 140 | Current Training Loss: 0.035736804635664594 | Error: 137.84185647964478%\n",
      "Epoch 140 | Current Validation Loss: 0.022935343726532224 | Error: 174.1317629814148%\n"
     ]
    }
   ],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    i = 300\n",
    "    # create imbeddings \n",
    "    embedded = convert_alpha_sequence(normalized_data_pairings_log(data_pairings))\n",
    "\n",
    "    # create data loader, we will train on the same one for consistency\n",
    "    train_loader, val_loader, test_loader = batching_fcn(dataset = embedded, split_proportion = [0.80, 0.18, 0.02], batch_size = batch_size, split_seed=30, batching_seed=42, balance = True)\n",
    "    for dropout_p in dropout:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for learning_rate in learning_rates:\n",
    "                print(f\"Chose learning rate {learning_rate}, number of epochs {num_epoch}, hidden_size {hidden_size}, batch size {batch_size}, dropout {dropout_p}\")\n",
    "                # creating the structures required for model\n",
    "                gru1 = GRU(input_size, hidden_size).to('cuda')\n",
    "                gru2 = GRU(input_size, hidden_size).to('cuda')\n",
    "                ann = ANN_dropout(hidden_size*2, dropout_p).to('cuda')\n",
    "                ann_noD = ANN(hidden_size*2).to('cuda')\n",
    "                # train\n",
    "                train(gru1, gru2, ann, train_loader, val_loader, batch_size=batch_size, num_epochs=num_epoch, learning_rate=learning_rate, plot=True, path=f\"./series{i:02d}\")\n",
    "                train(gru1, gru2, ann_noD, train_loader, val_loader, batch_size=batch_size, num_epochs=num_epoch, learning_rate=learning_rate, plot=True, path=f\"./series{i:02d}_nodropout\")\n",
    "    i += 1        \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
