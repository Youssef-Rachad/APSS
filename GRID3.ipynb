{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpnzRIAeFiUo"
   },
   "source": [
    "### GRU and ANN Training\n",
    "\n",
    "- Here I am going to experiment on the 3bp sequences\n",
    "- again, I want to see if overfitting can occur\n",
    "- I am using balancing, and am taking only positive non zero parts of the dataset for now\n",
    "\n",
    "AUTHORS: Youssef Rachad\n",
    "DATE: 03/31/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0vsOsRQg6OOH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dv-1yANG8kOp"
   },
   "outputs": [],
   "source": [
    "use_cuda = True \n",
    "device_id = 0  # choose the GPU device ID you want to use\n",
    "device = torch.device(f'cuda:{device_id}' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMupHVe4gYJD"
   },
   "source": [
    "#### Importing Batching Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dtTzWXBwgXme"
   },
   "outputs": [],
   "source": [
    "# custome made batching files to feed into our network (in file called batching.py)\n",
    "from batchingv7 import get_data_matrix, get_data_pairings,convert_alpha_sequence, batching_fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hckCFVtU1nCl",
    "outputId": "ada1f4fb-efcd-46ca-c6ba-cdcd9db79461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "# THIS IS WHERE THE DATA FILE IS IN THE FOLDER DOCUMENT\n",
    "data_file_name = 'FileP04_T4_18h_37C.xlsx'\n",
    "\n",
    "#importing the file and getting pairing\n",
    "data_pairings = get_data_pairings(get_data_matrix(data_file_name)) \n",
    "\n",
    "import random\n",
    "\n",
    "norm_pairings = data_pairings\n",
    "\n",
    "# create imbeddings \n",
    "embedded = convert_alpha_sequence(norm_pairings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "I4klF1M66fAI"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def get_abs_percent_error(y_pred, y_actual):\n",
    "  result = torch.mean(torch.abs((y_actual - y_pred) / y_actual))\n",
    "  #ipdb.set_trace()\n",
    "  return result\n",
    "\n",
    "def get_smape(y_pred, y_actual):\n",
    "  batch_smape = torch.mean(torch.abs(y_actual - y_pred) / ((torch.abs(y_actual) + torch.abs(y_pred))/2.0))\n",
    "  # STILL POSSIBILITY THAT BOTH y_pred AND y_actual are 0 (e.g., if our model \n",
    "  # makes very good predictions), and therefore get inf% error\n",
    "  # ==> THINK ABOUT HOW TO FIX THIS MORE PERMANENTLY\n",
    "  return batch_smape\n",
    "\n",
    "def plot_losses(iterations, training_loss, validation_loss, path):\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iterations, training_loss, label=\"Training\")\n",
    "    plt.plot(iterations, validation_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(path)\n",
    "    plt.clf()\n",
    "\n",
    "# Note: MAPE is Mean Absolute Percentage Error\n",
    "def plot_SMAPE(iterations, training_SMAPE, validation_SMAPE, path):\n",
    "    plt.title(\"Training Curve - SMAPE\")\n",
    "    plt.plot(iterations, training_SMAPE, label=\"Training\")\n",
    "    plt.plot(iterations, validation_SMAPE, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Symmetric Mean Average Percent Error\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(path)\n",
    "    plt.clf()\n",
    "\n",
    "def train(modelGru1, modelGru2, modelAnn, training_set, validation_set, batch_size, num_epochs=5, learning_rate=1e-4, plot=True, path=\".\"):\n",
    "    '''\n",
    "    modelGru1 : GRU for analysis of the top sequence\n",
    "    modelGru2 : GRU for analysis of the bottom sequence\n",
    "    modelAnn: For classiciation of the hidden state of the GRU\n",
    "    training_set: Batched [formatted properly] training set\n",
    "    validation_set: Batched [formatted properly] validation data\n",
    "    '''\n",
    "\n",
    "    # using mean squared error loss as this is not a classification problem\n",
    "    criterion = nn.MSELoss()\n",
    "    # NOTE: Might need to change depending on how the second model plays a role here\n",
    "    optimizer = optim.Adam(list(modelGru2.parameters()) +list(modelGru1.parameters()) + list(modelAnn.parameters()), lr= learning_rate) ## insert reference? \n",
    "\n",
    "    # recording the data\n",
    "    training_losses, validation_losses = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    training_SMAPE, validation_SMAPE = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    iterations = range(num_epochs)\n",
    "    \n",
    "    date_time = datetime.now().strftime(\"%m_%d_%H_%M\")\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    file_data = open(f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{num_epochs}_dropout{modelAnn.p}.txt\", \"a\")\n",
    "    file_data.write(f\"Chose learning rate {learning_rate}, number of epochs {num_epoch}, hidden_size {hidden_size}, batch size {batch_size}, dropout {dropout_p}\\n\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      # first sequence, second sequence, and then the affinity (ie the label)\n",
    "\n",
    "      train_total_loss = 0\n",
    "      val_total_loss = 0\n",
    "      train_total_error = 0\n",
    "      val_total_error = 0\n",
    "\n",
    "      num_train_batches = 0\n",
    "      num_val_batches = 0\n",
    "\n",
    "      # TRAINING LOOP\n",
    "      for seq1, seq2, label in training_set:\n",
    "        num_train_batches += 1\n",
    "      \n",
    "        # Forward pass through the two GRU (applied to the first and second sequency)\n",
    "        out, h1 = modelGru1(seq1.float().to(device))\n",
    "        out, h2 = modelGru2(seq2.float().to(device))\n",
    "        h1, h2 = h1.squeeze(0), h2.squeeze(0)\n",
    "\n",
    "        # concatenate the hidden output\n",
    "        h = torch.cat((h1, h2), dim=1).to(device)\n",
    "\n",
    "        # Forward pass through the ANN (which will be 256 inputs)\n",
    "        y_pred = modelAnn(h).squeeze().to(device)\n",
    "        # Compute the loss\n",
    "        loss = criterion(y_pred, label.float().squeeze().to(device))\n",
    "        train_total_loss += loss.item()\n",
    "        # ipdb.set_trace() # y_pred.shape: [10000], label.shape: [10000]\n",
    "        #if y_pred.shape != label.shape:\n",
    "          #ipdb.set_trace()\n",
    "        # train_total_error += get_abs_percent_error(y_pred, label.float())\n",
    "        train_total_error += get_smape(y_pred, label.float().to(device))\n",
    " \n",
    "        # Zero the gradients and perform backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # I am going to only track one of them for ease here\n",
    "        # print(\"         True Label: \" + str(label) + \" Predicted Output: \" + str(y_pred))\n",
    "      \n",
    "      training_losses[epoch] = train_total_loss/num_train_batches # /\n",
    "      training_SMAPE[epoch] = train_total_error/num_train_batches\n",
    "\n",
    "\n",
    "      # VALIDATION LOOP\n",
    "      for seq1, seq2, label in validation_set:\n",
    "        num_val_batches += 1\n",
    "        # Forward pass through the two GRU (applied to the first and second sequency)\n",
    "        out, h1 = modelGru1(seq1.float().to(device))\n",
    "        out, h2 = modelGru2(seq2.float().to(device))\n",
    "        h1, h2 = h1.squeeze(0), h2.squeeze(0)\n",
    "\n",
    "        # concatenate the hidden output\n",
    "        h = torch.cat((h1, h2), dim=1).to(device)\n",
    "\n",
    "        # Forward pass through the ANN (which will be 256 inputs)\n",
    "        y_pred = modelAnn(h).squeeze().to(device)\n",
    "        # Compute the loss\n",
    "        loss = criterion(y_pred, label.float().squeeze().to(device))\n",
    "        val_total_loss += loss.item()\n",
    "        #ipdb.set_trace()\n",
    "        val_total_error += get_smape(y_pred, label.float().to(device))\n",
    "      \n",
    "      validation_losses[epoch] = val_total_loss/num_val_batches\n",
    "      validation_SMAPE[epoch] = val_total_error/num_val_batches\n",
    "      \n",
    "      print(f\"Epoch {epoch} | Current Training Loss: {training_losses[epoch]} | Error: {training_SMAPE[epoch] * 100.0}%\")\n",
    "      print(f\"Epoch {epoch} | Current Validation Loss: {validation_losses[epoch]} | Error: {validation_SMAPE[epoch] * 100.0}%\")\n",
    "      file_data.write(f\"Epoch {epoch} | Current Training Loss: {training_losses[epoch]} | Error: {training_SMAPE[epoch] * 100.0}%\\n\")\n",
    "      file_data.write(f\"Epoch {epoch} | Current Validation Loss: {validation_losses[epoch]} | Error: {validation_SMAPE[epoch] * 100.0}%\\n\")\n",
    "      if epoch in [10, 50, 100]:\n",
    "        modelGru1_path = f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}\"\n",
    "        torch.save(modelGru1.state_dict(), modelGru1_path)\n",
    "        modelGru2_path = f\"{path}/3bp_{date_time}_model_Gru2_hiddensz{modelGru2.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}\"\n",
    "        torch.save(modelGru2.state_dict(), modelGru2_path)       \n",
    "        modelAnn_path = f\"{path}/3bp_{date_time}_model_Ann_hiddensz{modelGru2.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}\" \n",
    "        torch.save(modelAnn.state_dict(), modelAnn_path)\n",
    "        \n",
    "        plot_losses(iterations, training_losses, validation_losses, f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}_loss.png\")\n",
    "        plot_SMAPE(iterations, training_SMAPE, validation_SMAPE, f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}_SMAPE.png\")\n",
    "\n",
    "   \n",
    "    file_data.close()\n",
    "    # Save the current model (checkpoint) to a file\n",
    "    \n",
    "    modelGru1_path = f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}\"\n",
    "    torch.save(modelGru1.state_dict(), modelGru1_path)\n",
    "    modelGru2_path = f\"{path}/3bp_{date_time}_model_Gru2_hiddensz{modelGru2.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}\"\n",
    "    torch.save(modelGru2.state_dict(), modelGru2_path)       \n",
    "    modelAnn_path = f\"{path}/3bp_{date_time}_model_Ann_hiddensz{modelGru2.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}\" \n",
    "    torch.save(modelAnn.state_dict(), modelAnn_path)\n",
    "    \n",
    "    plot_losses(iterations, training_losses, validation_losses, f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}_loss.png\")\n",
    "    plot_SMAPE(iterations, training_SMAPE, validation_SMAPE, f\"{path}/3bp_{date_time}_model_Gru1_hiddensz{modelGru1.hidden_size}_batchsz{batch_size}_lr{learning_rate}_epoch{epoch}_dropout{modelAnn.p}_SMAPE.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "S4bwMcvM6TwD"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)  # initial hidden state\n",
    "        out, hn = self.gru(x)  # out shape: (batch_size, sequence_length, hidden_size), hn shape: (1, batch_size, hidden_size)\n",
    "        return out, hn\n",
    "        #return out[:, -1, :]  # return the last hidden state, shape: (batch_size, hidden_size)\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 20)  # 1st hidden layer with 64 nodes\n",
    "        self.fc2 = nn.Linear(20, 10)  # 1st hidden layer with 64 nodes\n",
    "        self.output_layer = nn.Linear(10, 1)  # Output layer with 1 node\n",
    "        self.p = 0\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))  # Apply ReLU activation to 1st hidden layer output\n",
    "        x = nn.functional.relu(self.fc2(x))  # Apply ReLU activation to 1st hidden layer output\n",
    "        output = self.output_layer(x)        # Output layer\n",
    "        return output\n",
    "\n",
    "class ANN_dropout(nn.Module):\n",
    "  def __init__(self, input_size, p):\n",
    "        super(ANN_dropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 20)  # 1st hidden layer with 64 nodes\n",
    "        self.fc2 = nn.Linear(20, 10)  # 1st hidden layer with 64 nodes\n",
    "        self.p = p\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.output_layer = nn.Linear(10, 1)  # Output layer with 1 node\n",
    "        \n",
    "  def forward(self, x):\n",
    "    x = nn.functional.relu(self.fc1(x))  # Apply ReLU activation to 1st hidden layer output\n",
    "    x = nn.functional.relu(self.fc2(x))  # Apply ReLU activation to 1st hidden layer output\n",
    "    x = self.dropout(x)\n",
    "    output = self.output_layer(x)  # Output layer\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "icLktHeG6L4V"
   },
   "outputs": [],
   "source": [
    "def normalized_data_pairings_log(list_pairs):\n",
    "    '''INPUT: A list of the data in full dataset, of the form:\n",
    "            [[\"AAA\", \"TTT\", num_binding_events0],\n",
    "                            ...\n",
    "             [\"AAC\", \"TTG\", num_binding_events1]]\n",
    "     OUTPUT: a normalized tuple array of the data (helps with comparision)'''\n",
    "    norm_list_pairs = []\n",
    "    # doing fancy log stuff\n",
    "    total_binding_events = np.log(np.array(list_pairs)[:,2].astype(int).sum())\n",
    "    for pair in list_pairs:\n",
    "      # doing fancy log stuff\n",
    "      affinity = float(np.log(pair[2] + 1)/total_binding_events) \n",
    "      assert type(affinity) == float\n",
    "      norm_list_pairs.append([pair[0], pair[1], affinity])\n",
    "    return norm_list_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYLs9aWd9Bke"
   },
   "source": [
    "Inputting Data and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yVkSVND92jOh"
   },
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqAvgban2vSD"
   },
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3-va9qX2xcf"
   },
   "outputs": [],
   "source": [
    "learning_rates = [1.0e-5]\n",
    "num_epoch = 250\n",
    "# GRU params\n",
    "input_size = 2 # size of the encoding\n",
    "hidden_sizes = [15, 30, 50] # number of hidden states in the GRU\n",
    "# ANN params\n",
    "dropout = [0.3]\n",
    "batch_sizes = [1, 5, 10] # batches in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYeLY4xj2y-T",
    "outputId": "0f805eee-1065-4c77-d5f3-4621461e30ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose learning rate 1e-05, number of epochs 2, hidden_size 15, batch size 1, dropout 0.1\n",
      "Epoch 0 | Current Training Loss: 0.021812733715467617 | Error: 107.94802904129028%\n",
      "Epoch 0 | Current Validation Loss: 0.014450430140124017 | Error: 159.882915019989%\n",
      "Epoch 1 | Current Training Loss: 0.021535595880022303 | Error: 106.44769668579102%\n",
      "Epoch 1 | Current Validation Loss: 0.014350013939918974 | Error: 160.10401248931885%\n",
      "Epoch 0 | Current Training Loss: 0.0290190809028469 | Error: 115.57183265686035%\n",
      "Epoch 0 | Current Validation Loss: 0.01693304132716885 | Error: 159.52410697937012%\n"
     ]
    }
   ],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    i = 200\n",
    "    # create imbeddings \n",
    "    embedded = convert_alpha_sequence(normalized_data_pairings_log(data_pairings))\n",
    "\n",
    "    # create data loader, we will train on the same one for consistency\n",
    "    train_loader, val_loader, test_loader = batching_fcn(dataset = embedded, split_proportion = [0.80, 0.18, 0.02], batch_size = batch_size, split_seed=30, batching_seed=42, balance = True)\n",
    "    for dropout_p in dropout:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for learning_rate in learning_rates:\n",
    "                print(f\"Chose learning rate {learning_rate}, number of epochs {num_epoch}, hidden_size {hidden_size}, batch size {batch_size}, dropout {dropout_p}\")\n",
    "                # creating the structures required for model\n",
    "                gru1 = GRU(input_size, hidden_size).to('cuda')\n",
    "                gru2 = GRU(input_size, hidden_size).to('cuda')\n",
    "                ann = ANN_dropout(hidden_size*2, dropout_p).to('cuda')\n",
    "                ann_noD = ANN(hidden_size*2).to('cuda')\n",
    "                # train\n",
    "                train(gru1, gru2, ann, train_loader, val_loader, batch_size=batch_size, num_epochs=num_epoch, learning_rate=learning_rate, plot=True, path=f\"./series{i:02d}\")\n",
    "                train(gru1, gru2, ann_noD, train_loader, val_loader, batch_size=batch_size, num_epochs=num_epoch, learning_rate=learning_rate, plot=True, path=f\"./series{i:02d}_nodropout\")\n",
    "    i += 1        \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
